{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and LSTM\n",
    "#### By MMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print('%s' % (txt,), end='')\n",
    "\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0 / vocab_size) * seq_length\n",
    "\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01  # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01  # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01  # hidden to output\n",
    "    b = np.zeros((n_a, 1))  # hidden bias\n",
    "    by = np.zeros((n_y, 1))  # output bias\n",
    "\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)  # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)\n",
    "\n",
    "    return a_next, p_t\n",
    "\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next']\n",
    "    daraw = (1 - a * a) * da\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b'] += -lr * gradients['db']\n",
    "    parameters['by'] += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "\n",
    "        # One step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t - 1], x[t])\n",
    "\n",
    "        # Update the loss\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "\n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "    return gradients, a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters :  19909 , Number of unique characters : 27\n"
     ]
    }
   ],
   "source": [
    "data = open('data/dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Total Characters :  %d , Number of unique characters : %d' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = {ch: i for i, ch in enumerate(sorted(chars))}\n",
    "index_to_char = {i: ch for i, ch in enumerate(sorted(chars))}\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clipping Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_gradient(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'],\\\n",
    "                                gradients['db'], gradients['dby']\n",
    "   \n",
    "    # clipping\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, a_min=-1 * maxValue, a_max=maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_index, seed):\n",
    "\n",
    "    # fetch parameters\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], \\\n",
    "                           parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # create one-hot vector x\n",
    "    x = np.zeros(vocab_size)\n",
    "    x = np.reshape(x, (vocab_size, 1))\n",
    "\n",
    "    a_old = np.zeros(n_a)\n",
    "    a_old = np.reshape(a_old, (n_a, 1))\n",
    "    \n",
    "    indices = []\n",
    "\n",
    "    counter = 0\n",
    "    newline_char = char_to_index['\\n']\n",
    "\n",
    "    index = -1\n",
    "    # forward phase\n",
    "    while index != newline_char and counter != 50:\n",
    "        a = np.tanh(np.matmul(Wax, x) + np.matmul(Waa, a_old) + b)\n",
    "        z = np.matmul(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        y = y.flatten()\n",
    "\n",
    "        np.random.seed(counter + seed)\n",
    "\n",
    "        # sampling from index set\n",
    "        index = np.random.choice(27, p=y)\n",
    "\n",
    "        indices.append(index)\n",
    "        x = np.zeros(27)\n",
    "        x = np.reshape(x, (vocab_size, 1))\n",
    "        x[index] = 1\n",
    "        a_old = a\n",
    "\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    if counter == 50:\n",
    "        indices.append(char_to_index['\\n'])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_old, parameters, learning_rate = 0.01):\n",
    "    # Forward phase\n",
    "    loss, cache = rnn_forward(X, Y, a_old, parameters, vocab_size)\n",
    "    \n",
    "    # Backpropagation\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clipping\n",
    "    gradients = clip_gradient(gradients, 5)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data, index_to_char, char_to_index, num_iterations=16000, n_a=50, dino_names=10, vocab_size=27):\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    a_old = np.zeros((n_a, 1))\n",
    "\n",
    "    with open(\"data/dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "\n",
    "    shuffle(examples)\n",
    "\n",
    "    # Optimization loop\n",
    "    iteration = []\n",
    "    losses = []\n",
    "    for j in range(num_iterations):\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_index[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_index[\"\\n\"]]\n",
    "\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_old, parameters, learning_rate=0.01)\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            losses.append(loss)\n",
    "            iteration.append(j)\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters, char_to_index, seed)\n",
    "                print_sample(sampled_indices, index_to_char)\n",
    "                seed += 1\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "    return parameters, iteration, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 32.978147\n\nNkzxwtdmfqoeyhsqwasjkjvu\nKneb\nKzxwtdmfqoeyhsqwasjkjvu\nNeb\nZxwtdmfqoeyhsqwasjkjvu\nEb\nXwtdmfqoeyhsqwasjkjvu\nB\nWtdmfqoeyhsqwasjkjvu\n\n\n\nIteration: 100, Loss: 33.735805\n\nOlyvusbnerpdyhsru\nKoea\nLyvusbnerpdyhsru\nOea\nYvusbnerpdyhsru\nEa\nVusbnerpdyhsru\nA\nUsbnerpdyhsru\n\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200, Loss: 34.150962\n\nOmxuusaocrpbwgsru\nLoca\nMxuusaocrpbwgsru\nOca\nYuusaocrpbwgsru\nDa\nUusaocrpbwgsru\nA\nUsaocrpbwgsru\n\n\n\nIteration: 300, Loss: 34.376265\n\nOlvutsaocrobuhspt\nKod\nLwutsaocrobuhspt\nOd\nXutsaocrobuhspt\nDa\nUtsaocrobuhspt\nA\nUsaocrobuhspt\n\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 400, Loss: 34.506741\n\nOnwutsarasoguosru\nKoea\nLxutsarasoguosru\nOea\nXutsapdsoguosru\nDa\nUtsapdsogunsru\nA\nUs\n\n\n\nIteration: 500, Loss: 34.246085\n\nOnxusihierreuosguauraqst\nKnda\nLyusjgnbronurqsp\nOga\nYusigmcronurpsoarnomtt\nEa\nUskeoerphurspuaurapst\nA\nUs\n\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600, Loss: 33.959585\n\nNiyusieogoraweshuburhnsoaltiyheaiceucakaol\nInca\nKyusieogoraweshuburhnsoaltiyheaiceucakaol\nNca\nYusieogoraweshuburhnsoaltiyheaiceucakaol\nDa\nUsieogoraweshuburhnsoaltiyheaiceucakaol\nA\nTrcncoraxeros\n\n\n\nIteration: 700, Loss: 33.550347\n\nNixttqbndoravdshudus\nIoha\nJytsohheronurusdbroreshaosawihalahtaehaol\nNda\nYtsohheronurusdbroreshaosawihalahtaehaol\nDa\nUsegjeronurusdbroreshaosawihalahtaehaol\nA\nTpcldoravdshudus\n\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 800, Loss: 32.994344\n\nNiytsohieronurusahrurosaerusus\nInebaerur\nJytsofneronurusahrurosaerusus\nNebaerur\nYusiepdoraurusahrurosaerusus\nDaadosanasososaus\nUsiepdoraurusahrurosaerusus\nAadpsaiaus\nToeneronurusahrurosaerusus\n\n\n\nIteration: 900, Loss: 32.517798\n\nNixtsieohos\nIndaadosanatsaurusis\nJytrogigos\nNdaadosanatsaurusis\nYtrofifos\nDaadosanatsaurusis\nUshepcoraurusadrtihtpaos\nAadosalatsaurusis\nToemcoraurusadrtihtpaos\n\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Loss: 32.188967\n\nNkytsngneronupsaudur\nJmecafrup\nKytsnepepojuosaugus\nNecafruocaos\nYttoglepolupros\nEcaerunaantis\nUsmepepoiunus\nAaerunaantis\nToglepoiunus\nAfrunaantis\n\n\nIteration: 1100, Loss: 31.620715\n\nMhytppanbos\nIndaaeronaaosaurusis\nIytppancoraurus\nMcaaeronaaosaurushurut\nYtroencopaurus\nDaaeronaantis\nUsgdlaopaurus\nAaerolaansauruseurut\nToemaopaurus\nAerolaanrhsaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1200, Loss: 31.362329\n\nMhytrolidne\nIndaahos\nIxtqrapcos\nMbaagosan\nYtroipcos\nDaagosan\nUsicidos\nAafpsaiaus\nToglasaus\n\n\n\nIteration: 1300, Loss: 30.976954\n\nNixtromncos\nInecaison\nKytpsaurus\nNdaahosan\nYtqrapgsaus\nDaagosan\nUsicneronus\nAagosalasous\nTogmcosaurus\nAfronacnus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1400, Loss: 30.546608\n\nMgytosaurus\nImacagos\nIytosaurus\nMacagosaurus\nYtpraolos\nCaagosas\nTqranasaus\nAagosaohus\nToilarootas\nAgosaiaus\n\n\nIteration: 1500, Loss: 30.266591\n\nNixtrolleronus\nJldaberrecaps\nKytrolleronus\nNecakrpecaosaurus\nYusleperolus\nDaakosanasius\nTrokleromus\nAairtedantos\nTohiepomurus\nAhosaiarsaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1600, Loss: 29.881557\n\nMhytromierieurmmtasaurus\nInecahosam\nKytrolierlaurus\nMcaahosakaus\nYusiciesaus\nDaahosaiaus\nTroliepolus\nAahosaiaus\nTogieoraurus\nAgpsaedos\n\n\nIteration: 1700, Loss: 29.615379\n\nMhxusicnesausaurun\nInecaisomacmusaurus\nIyusicnesausaurun\nMbaahosaoepsaurusaurut\nYusiclesausaurun\nCaahosanasous\nUsicigoraurus\nAagosanasous\nTogieoraurus\nAhosalaptosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1800, Loss: 29.251150\n\nMhytroeoloraurus\nImecahosalapsaurus\nIytroeoloraurus\nMacahosahapsaurus\nYuskererolurosanrus\nCaberomacosaurus\nTrodlerlaurus\nAagromacosaurus\nTohierolursauius\nAgrrecansaurus\n\n\nIteration: 1900, Loss: 29.137524\n\nLivtosaurus\nImecagosaodos\nIxusodomoraurus\nLecagosan\nXusodnasaus\nCaaersadassosaurus\nToranerolus\nAagosaiaus\nTodidosaurus\nAgosaecosaurusiurax\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000, Loss: 28.868081\n\nMhysosaurus\nIlecagosaurus\nIxusibidosaurus\nMcaaeropa\nYusicieriaurus\nCabcosalatrus\nToranerehuroselopensa\nAafosamatros\nTogidosaurus\nAgosaiaus\n\n\nIteration: 2100, Loss: 28.562347\n\nMewtosaurus\nHhabagosataus\nIxuseepasaus\nMacagosas\nYushanesausaurulophesa\nCaagosan\nToraneriaurus\nAagosamasius\nTogiasaus\nAgrona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2200, Loss: 28.301059\n\nMhysrunador\nImedagrur\nIytoranern\nMcabdosasaus\nYusnerisaurus\nDabdosanbos\nTorangos\nAagron\nTogleromurus\nAgrona\n\n\nIteration: 2300, Loss: 27.998638\n\nMixusieras\nJlecagrus\nKytrohgdos\nMecagrsaogsgurus\nYuspconsaurus\nDaagpsan\nToraneriatgrns\nA\nTogmapontarps\nAgrun\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2400, Loss: 27.727147\n\nNixusolonosaurus\nKnecaisin\nKytosaurus\nNecaismur\nYusogonosaurus\nDaagosapaus\nToraomosaurus\nA\nTohkeronurus\nAhosaiatortengus\n\n\nIteration: 2500, Loss: 27.459052\n\nMhytosaurus\nJicaagosapaus\nKytosaurus\nNbaagosanaskus\nXuspandosaurus\nCaagosalasius\nTosaomis\nA\nTogidosaurus\nAgroodapsaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2600, Loss: 27.234726\n\nMewtosaurus\nIlbaagrsanaus\nIxtosaurus\nMacagrphacosaurus\nXusnbnesausus\nCaadosaurus\nTorangosaurus\nAaeronacosaurus\nTodidosaurus\nAgprecansaurus\n\n\nIteration: 2700, Loss: 27.046437\n\nMewusiandor\nJiacaisaurasossur\nKwsosaurus\nMacagosaupus\nXusiandlomurpsicrsauruc\nDaaeroma\nTorangosaurus\nAaeromabosaurus\nTogibopaurus\nAgosalaptosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2800, Loss: 26.930221\n\nMeytorapeus\nIkacalosaurus\nKyusianes\nMacakosaurus\nXusmanesaurus\nCaagosaurus\nToranernathus\nAagosapdus\nToimapreuresaurus\nAhosaiclus\n\n\nIteration: 2900, Loss: 26.767613\n\nNhytromigps\nLicacespur\nLytrolignn\nNacalosaurus\nXustdonos\nDaagpsas\nTorangos\nAagpsan\nTohierontes\nAistelasius\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3000, Loss: 26.586818\n\nMixusoiliponteritas\nJicaaeslia\nKwusraneronuratras\nMecaison\nXusraneromurastarihius\nDaaeroma\nTrocheoraurus\nAagroma\nToimapomuratras\nAjsrgachurur\n\n\nIteration: 3100, Loss: 26.395901\n\nMevtosaurus\nKicbagosaurus\nLustolonosaurus\nMacaisona\nXusrangosaurus\nDaagosaurus\nTorangosaurus\nAagosaurus\nToglepomus\nAhosaicosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3200, Loss: 26.270996\n\nMhyusoindopaurus\nImacalosaurus\nJwtroencionus\nMacakosapaus\nYustandopaurus\nCaadosapbus\nTorangoraurus\nAagosan\nTogkaoraurus\nAjqtedanthus\n\n\nIteration: 3300, Loss: 26.097702\n\nLiwtromemps\nHidaagosaurus\nIwtrolicir\nLacalosaurus\nXustanatorus\nCabatradaptis\nTqraneralurantar\nAagosaurus\nToiolosaurus\nAistelastos\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3400, Loss: 25.978487\n\nMivushorasaus\nJojaaishol\nKustoloncilus\nMcabdos\nXusterasaus\nCabdosaurus\nTruhonos\nAafsor\nTokolos\nAkrur\n\n\nIteration: 3500, Loss: 25.940683\n\nMhyusogonmiaurus\nImecagosaurus\nIwusoinchurus\nMacahosaurus\nYussaogosaurus\nDabcosaurus\nTrucihinaurus\nAadosaurus\nTododopaurus\nAiruia\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3600, Loss: 25.807557\n\nMewustandosaurus\nHibaagton\nIxusohicirathus\nMabagrus\nYusoenesausus\nCaaerteechurus\nTrochepngvesaurus\nAaerracaptosaurus\nToendoraurus\nAistedaothus\n\n\nIteration: 3700, Loss: 25.661207\n\nMewtrogjaronteritarasaur\nHeicalosaurus\nIxtrodongonus\nMacahosan\nYusrangosaurus\nCabdpsanaptosaurus\nTrodomosaurus\nAaerphachusilioton\nToeneronteros\nAgrrgcanthus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3800, Loss: 25.492174\n\nMexusolonis\nIlecalosaurus\nJytroeonionus\nMacagpsaurus\nYuspanchonus\nDaaerona\nTrodneronus\nAaerona\nToenchonusauris\nAhosaiaus\n\n\nIteration: 3900, Loss: 25.345981\n\nNiwushoran\nKolaakosaurus\nLytroknbopeus\nNdaagrra\nXustangos\nDaagrra\nTrodonnonus\nA\nTohicoraurus\nAkrula\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4000, Loss: 25.194842\n\nMixusoenephgurorocophosaloravmi\nIlecaeroma\nJyusocheplhunorocophosamorawkodidetadia\nMacagpria\nXusocheplgunorocophosamorawkodidetadia\nCaacorcachurupenton\nToramckoiuosaurus\nA\nToenepicurorocophosaloravmi\nAgrtecaptosaurus\n\n\nIteration: 4100, Loss: 25.120117\n\nMewusolops\nIlecakosaurus\nJwusoendophus\nMacaisaurus\nXuspcilosaurus\nDaafosaurus\nTrocheoravhus\nA\nTodleoravisaurus\nAisteedosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4200, Loss: 24.966579\n\nMewuspendoravirmufus\nJicaberthachurus\nKwstodonnonvisaurus\nMacagrreharros\nXuspbngosaurus\nDaadosaleps\nTorangosaurus\nA\nTodidoravlps\nAgpscedosaurus\n\n\nIteration: 4300, Loss: 24.937116\n\nMavuspanesaurus\nKiecagsmidanlosaurus\nLytrodomosaurus\nMacaisii\nXusranesaurus\nDaagosan\nTrocidosaurus\nA\nToglepomus\nAistedanthus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4400, Loss: 24.863292\n\nMcwuslengosaurus\nJiacalosaurus\nKustolonosaurus\nMacaishelanthus\nXusodonmnaurasqereonos\nDaagosaurus\nTorangosaurus\nA\nToenchomus\nAiroma\n\n\nIteration: 4500, Loss: 24.684382\n\nMevussangtieus\nJiacalropb\nKustraonis\nMacalropa\nXustangosaurus\nDaagrool\nTorangosaurus\nA\nToglaosaurus\nAirona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4600, Loss: 24.716086\n\nNgyuslenes\nKibaahosaurus\nLytromidos\nNacajshia\nXustanesaurus\nDaagosas\nTrochasaurus\nAaespeedos\nTranermethospanis\nAlrona\n\n\nIteration: 4700, Loss: 24.585943\n\nMeyusoikinonveros\nHeebagosaurus\nIvrosaurus\nMabaesiceiaurus\nYuspaneosaurus\nCaachpadanthur\nTrocheoreyisaurus\nAaerona\nTodiatonyitos\nAhosalatops\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4800, Loss: 24.560384\n\nMevtrolmeroptor\nHelbakptidadsaurus\nIvustandor\nMacalstel\nXustcilisaurus\nCabcosaurus\nTrodolophur\nAadrona\nTraneronteritar\nAkroma\n\n\nIteration: 4900, Loss: 24.460483\n\nMevuslencioptoratesaurus\nJibaadosaurus\nKustrepanoptoratesaurus\nMacaisilechus\nXuspbolopeus\nCabatria\nTrocheopdurus\nA\nToencipatisaurus\nAgrokachus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5000, Loss: 24.532100\n\nMeutrolonos\nImeeagptha\nJustrchhisaurus\nMacalrtel\nXustengosaurus\nDabdropa\nTrodomosaurus\nAagropa\nTogndosaurus\nAgrondanthus\n\n\nIteration: 5100, Loss: 24.391821\n\nMewtosaurus\nHiccaeropa\nIvrrodona\nMacahosaurus\nYurockipomus\nCaadosaurus\nTrocherlctis\nAadosaurus\nToeoerias\nAgrola\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5200, Loss: 24.370715\n\nLixusiangosaurus\nHmacaishia\nHustodonisaurus\nLabagosaurus\nYusodonkolus\nCaadosaurus\nTrocheopeuptos\nAadosaurus\nTodlhorexaurus\nAgruhachonsaurus\n\n\nIteration: 5300, Loss: 24.323322\n\nMexusolngosaurus\nIlebagosaurus\nJyusodongonus\nMacagrona\nYustangosaurus\nDaadosanbosaurus\nTrodlisaurus\nA\nTodigosaurus\nAgrrgcantisaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5400, Loss: 24.148886\n\nMexusolopionv\nHicaadosaurus\nIxtrogoniontaosaurus\nMacaesia\nYustapanoptosaurus\nCaadosaurus\nTrochasaurus\nA\nTodndosaurus\nAgrpeecesaurus\n\n\nIteration: 5500, Loss: 24.070656\n\nMexuselna\nHicaacosaurus\nIwrpsaurus\nMacaesaurus\nYusochasaus\nCaacosaurus\nToraolosaurus\nA\nTodokiolx\nAgroma\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5600, Loss: 24.021891\n\nNixusmanesaurus\nKlebafropderus\nLwusogoniolus\nNebagron\nXussanatons\nDaaeropa\nTrocheoraxaurulnanaus\nBaisona\nToenasaurus\nAjroma\n\n\nIteration: 5700, Loss: 23.865180\n\nLgytoraphus\nHeicalosaurus\nIustodon\nLacagosaurus\nXusrangosaurus\nCabbosaurus\nTroclosaurus\nAadrona\nTodokopeus\nAgroga\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5800, Loss: 23.930134\n\nMavusnanesaurus\nKidaagosaurus\nKurtogichlaterataps\nMacalosateps\nXuspanesaurus\nDabarsaldos\nTrocharomurlos\nAaerona\nTohairicpisaurus\nAjrola\n\n\nIteration: 5900, Loss: 23.914447\n\nMewtosaurus\nKicaaerope\nKwrrocheromus\nMacaisig\nXusrangosaurus\nDabbosaurus\nToramaor\nA\nTogmaseltes\nAhosaicis\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6000, Loss: 23.847379\n\nMavusiajing\nIeiaaesaurus\nJytrodonateus\nMacalpsaurus\nXustdonionx\nCabarona\nTrocharonus\nA\nTohelosaurus\nAjrona\n\n\nIteration: 6100, Loss: 23.825514\n\nMewtosaurus\nIleaaeropebeusoonotheawa\nJustodonhomus\nMacaeropechurus\nXussangosaurus\nDaacosaurus\nTrocharoptosaurus\nAacjondentitematop\nTranckontisaurus\nAkroma\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6200, Loss: 23.770347\n\nMeutosaurus\nJicbaesaurus\nKustodongnctoras\nMacaisaurus\nXustanesaurus\nCabarosaurus\nTrochasaurus\nAagosaurus\nTodolosaurus\nAlrona\n\n\nIteration: 6300, Loss: 23.734409\n\nLextosaurus\nHeibaisen\nHustodonosaurus\nLacaispeiatros\nYusrangosaurus\nBaackoma\nTrochchibuosaurus\nAadrooddosaurus\nTodolosaurus\nAissaedithus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6400, Loss: 23.604027\n\nLivrrodon\nHodebcosaurus\nHutrratiramthtludus\nLacaissaggus\nXustholosaurus\nCabersasaurus\nTrochisaurus\nAbhosaurus\nTrangosaurus\nAgrricaptos\n\n\nIteration: 6500, Loss: 23.699036\n\nMgyusodon\nHiceadosaurus\nIustodonatius\nMacalosaurus\nXuspangosaurus\nCabcosaurus\nTorengosaurus\nA\nTodolophus\nAgroka\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6600, Loss: 23.762017\n\nMevusoenesaurus\nImecalosaurus\nJyuspanfsaurus\nMacaispeedithus\nYuspanesaurus\nDaadosaurus\nTrodleriaterataps\nAadosaurus\nToendopetatos\nAhppeiansaurus\n\n\nIteration: 6700, Loss: 23.686580\n\nMawusoia\nHiabaesaurus\nHyusodicus\nMabaesia\nYusodia\nCaacisasaus\nTrochasaurus\nAadosaurus\nTodichiaurus\nAgruia\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6800, Loss: 23.609673\n\nMewusellachitaros\nIlacanrus\nJyusodon\nMacaesaurus\nYusrapanosiosaurus\nDabaspelanops\nTrrasaurus\nA\nTodidos\nAgssagaptos\n\n\nIteration: 6900, Loss: 23.541755\n\nMixusodonconteros\nIlecagps\nJyusodongontanguius\nMacagosaurus\nYustdngsaurus\nDaadosaurus\nTordidon\nA\nTodleniaurus\nAgpsdachusmoneus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7000, Loss: 23.531944\n\nMgyutoerisaurus\nImacalosaurus\nJyutodonsaurus\nMacaltona\nYustaphorhylus\nDaagrtha\nTrodonsaurus\nA\nToengtiius\nAisthacosaurus\n\n\nIteration: 7100, Loss: 23.425732\n\nMixusohia\nJicabasaurus\nKytosaurus\nMacaisaurus\nXuspenasaurus\nCabbosaurus\nTorangosaurus\nA\nToendosaurus\nAgrona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7200, Loss: 23.375400\n\nMexuslingiphus\nHeecalosaurus\nIustrisaurus\nMacagrtidbnposaurus\nXuspconosaurus\nCabbosaurus\nTrocieroptosaunus\nA\nTocheosaurus\nAeronceprosaurus\n\n\nIteration: 7300, Loss: 23.295572\n\nMavuskina\nJlacalosaurus\nKustodon\nMacaisaurus\nXusodon\nDaadosaurus\nTrocheroptos\nA\nToemanosiosaurus\nAhosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7400, Loss: 23.351734\n\nMavusoflesaurus\nJoecalrtel\nKustodonnncurhos\nMacaisheianthus\nXussangosaurus\nDaadroncansaurus\nToramanontesaurus\nA\nTodichilus\nAjpola\n\n\nIteration: 7500, Loss: 23.369197\n\nMawusoenes\nJiacaisaurus\nKytrochaqurus\nMacaisaurus\nXusrangosaurus\nDaaeroodanus\nTosaneosaurus\nA\nToeneromus\nAissaeeosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7600, Loss: 23.304235\n\nMixusoenitheus\nIlecagosaurus\nJustoeonnonus\nMcabcosaurus\nXusoengosaurus\nDabcosaurus\nTorameosaurus\nA\nToemepiausaurus\nAipomachus\n\n\nIteration: 7700, Loss: 23.297070\n\nMevusoengsaurus\nKicbaespe\nLustogonnosigeus\nMacahosaurus\nXusrangosaurus\nDabarrahashus\nTrochenhathosaurus\nAbcosaurus\nTodkeosaurus\nAjprgadosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7800, Loss: 23.252255\n\nMewusania\nHiceajosataus\nIvusegoplomylus\nMacalosaurus\nYuspanchohur\nCabasrefa\nTrocngeonx\nAbcosateramur\nTofomon\nAjrona\n\n\nIteration: 7900, Loss: 23.180493\n\nLevusianesaurus\nGlacalosaurus\nHustrapeps\nLacaiscelator\nXusranesaurus\nBabastaddos\nTrodominathus\nAbbrradanthus\nTodilosaurus\nAgrthachus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8000, Loss: 23.191877\n\nMeutosaurus\nJicebbosaurus\nKustreolophus\nMacalosaurus\nXustangosaurus\nCabdosaurus\nTrochenoosaurus\nAberoma\nToenasaurus\nAgrukehator\n\n\nIteration: 8100, Loss: 23.239496\n\nMewushangosaurus\nHicabauroka\nIusosaurus\nMacairus\nYusodonisaurus\nCabasoracisaurus\nTosaurus\nAadosaurus\nTodiaronusirocondesia\nAgpsanarusaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8200, Loss: 23.234356\n\nMewusodon\nHiebaiton\nIvusnandos\nMacakosaurus\nYutohenitetatos\nCabasteia\nTrocheronthus\nAbbosataton\nTogilosaurus\nAisteechus\n\n\nIteration: 8300, Loss: 23.230293\n\nMawuspangosaurus\nHehaagrrabaptos\nIvrspanesaurus\nMadaersasaurus\nYusraonosaurus\nCabbrredanosaurus\nTrocherontesaurus\nAadropa\nToeneroptords\nAgrrhachusaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8400, Loss: 23.173972\n\nNixusolonosaurus\nJohaaiosaurus\nKustrargosaurus\nNecajosaurus\nYuspanasaurus\nDaadosaurus\nTrodonosaurus\nAaerondansaurus\nTodomoraxirslerdmespa\nAironcanosaurus\n\n\nIteration: 8500, Loss: 23.147584\n\nNixustan\nKolaalosaurus\nLytrranilonthoptathieus\nNedalosaurus\nYusteonjonus\nDabersaurus\nTrrangonex\nBaktrla\nTognerontisaurus\nAlton\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8600, Loss: 23.019438\n\nMewusaurus\nIlacalosaurus\nKuspranciravhus\nMacairus\nXusodendontosaurus\nCaaeron\nTorandon\nA\nToceleravhus\nAgrona\n\n\nIteration: 8700, Loss: 23.069066\n\nMewusoeonosaurus\nInebaerosaurus\nJustreonosaurus\nMacaerope\nXutoeomophurus\nDaberondantosaurus\nTrodoerontis\nAberondantosaurus\nTodiforaxmus\nAgrria\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8800, Loss: 22.925843\n\nMawuskenesaurus\nIlecamosaurus\nJutprapsaurus\nMacagrus\nXutocheosaurus\nCabaspheesaurus\nTrocheps\nA\nTochaosaurus\nAfsticaptosaurus\n\n\nIteration: 8900, Loss: 23.015467\n\nMautosaurus\nLidaaishiderlis\nLutrrarcis\nMacaispelasis\nXutojiisaurus\nDaagsteiarsaurus\nTroengrneusauros\nBahuseibis\nTognernaurus\nAjsthachus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9000, Loss: 22.960458\n\nMawussasaurus\nJibaalosaurus\nKustonichons\nMacalroraclus\nXuspangosaurus\nDabarreleptes\nTorannosaurus\nA\nToenesaurus\nAhprncansaurus\n\n\nIteration: 9100, Loss: 22.911551\n\nMewusoenatilus\nJibaaesicados\nKustrarascitas\nMacaisie\nXustdorilaurus\nCabastelans\nTosarasaurus\nA\nTicheroosaurur\nAisteia\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9200, Loss: 22.975948\n\nMevushanes\nKolaalosaurus\nKususaurus\nMadalosaurus\nXustchanontiosaurus\nDabbosaurus\nTrrandon\nAbdosaurus\nTrancirax\nAlstelanoron\n\n\nIteration: 9300, Loss: 22.889756\n\nMewusodon\nHeecalosaurus\nIutosaurus\nMacaisoledlthtelosaurus\nYutoeonis\nCaacosaohus\nTrocheosaurus\nAafosaraptor\nTodoeosaurus\nAjroneasiosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9400, Loss: 22.890382\n\nMevtrodon\nHiceceropechus\nIustropeosaurus\nMadalosaurus\nYusteodon\nCabcosaurus\nTrodomialus\nAbdosaurus\nTohelopeustos\nAlptodenthus\n\n\nIteration: 9500, Loss: 22.850068\n\nMivusndonns\nImacaishia\nJustrengpneus\nMacalrona\nXusphieromus\nCaberrhachus\nTroconnohylops\nAbhosaurus\nToengpilus\nAgroleestosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9600, Loss: 22.975628\n\nMevtroinatlosaurus\nHhacaisaurus\nIusslangosaurus\nMacaisicelator\nXosrangis\nCabcisaurus\nToranginaverataps\nAagropa\nTianasaurus\nAgrona\n\n\nIteration: 9700, Loss: 22.925530\n\nLevrtonianosthosaurus\nHeiaadosaurus\nHytrocenites\nLacaisaurus\nYusodianorus\nCabbosaurus\nTosaurosaurus\nAbbosaurus\nTiamaronus\nAgrona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9800, Loss: 22.935292\n\nMexusmenisaurus\nHofaalosaurus\nIvusnengosaurus\nMacaiton\nYutohihionusauris\nCaaeroncepsaurus\nTrtenhosaurus\nAagosaurus\nTogngopathus\nAgrriachusaurus\n\n\nIteration: 9900, Loss: 22.905856\n\nMexusodon\nIlabaersasaurus\nJyusodondonus\nMacadosaurus\nYuspanesaurus\nDaacisaurus\nToranciraverater\nAadosaurus\nTodidon\nAfroledosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10000, Loss: 22.828054\n\nMexyvoenatops\nHicebdosaurus\nIxusteomia\nMacagropa\nYuspengosaurus\nCaberte\nTrrcheosaurus\nAadrondalsaurn\nTodoepomus\nAgrrha\n\n\nIteration: 10100, Loss: 22.799093\n\nMiwusnephiddy\nImacaesaurus\nJyusodongontesaurus\nMacaisaurus\nYusodongontgosagros\nCaadropedator\nTrodolosaurus\nAbersaurus\nToeneroptiros\nAlroodentis\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10200, Loss: 22.780913\n\nNgyuspanasaurus\nKojaalosaurus\nLustogianoptirus\nNabajosaurus\nXuspanasaurus\nDabarrag\nToranernaurus\nAacnradaptor\nToenboraxiosaurus\nAisteeanosaurus\n\n\nIteration: 10300, Loss: 22.671966\n\nMawusiangosaurus\nHegaagosaurus\nJuspodon\nMacagrope\nXusocephalus\nCabarradeps\nTrochapnaus\nAbbrona\nToceloraxeus\nAgroma\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10400, Loss: 22.691692\n\nMawuspandos\nKolaaisaurus\nLustoimeroshus\nMacalosaurus\nXuspandosaurus\nDabastelasaurus\nTrochanoltesaurus\nBagpondenthus\nToenerneverataps\nAjpoma\n\n\nIteration: 10500, Loss: 22.729484\n\nMawustar\nJiacalosaurus\nKustoglesaus\nMacaisia\nYutoglatong\nDabatrodanthus\nToramaosaurus\nA\nTichapomus\nAgpria\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10600, Loss: 22.731536\n\nMavusodon\nKidabaron\nLytrodoneraus\nMacalosaurus\nYussaperalups\nDaagrona\nTrodonosaurus\nA\nTokigopeurus\nAkrone\n\n\nIteration: 10700, Loss: 22.711873\n\nMexusochirus\nJicaacosaurus\nKustolongonus\nMacafropechurus\nXutocheromus\nDabaropeditos\nTrocheronthosaurus\nA\nTogianiausauralophus\nAjprecaps\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10800, Loss: 22.695047\n\nMewuspanesaurus\nJicaaessa\nKustrasaurus\nMacaessaelosaurus\nYuspanchidus\nDabassaeiroshurus\nTrncheroptorex\nAbbosaurus\nToemasaurus\nAkropa\n\n\nIteration: 10900, Loss: 22.620520\n\nMewusdeomhelus\nHidacerathas\nIustonlepraus\nMacalosaurus\nYusrapsaurus\nCabastelaptor\nTrocenatos\nAbcosasaurus\nToenesaurus\nAlroodenstthanthelya\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11000, Loss: 22.558825\n\nLevtosaurus\nHicaceratidlurus\nHustrerasaurus\nLacaisaurus\nYuspanesaurus\nCabatrna\nTroceosaurus\nAbauraedltgs\nToficholus\nAgronachus\n\n\nIteration: 11100, Loss: 22.608859\n\nMhyusochipneus\nHicabdosaurus\nIussuchipolus\nMecakosaurus\nXuspangosaurus\nCabcosaurus\nTorengosaurus\nAbcosaurus\nToenesceusaurus\nAgroma\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11200, Loss: 22.726995\n\nMewushangosaurus\nHiceadosaurus\nIvusianesaurus\nMacairus\nYusianesaurus\nDabatria\nTorapenmashuruglephus\nAbasoia\nToencopatausuchus\nAgpnodanthus\n\n\nIteration: 11300, Loss: 22.696876\n\nLewuskanisaurus\nHeeaaersasaurus\nHyrosaurus\nLabaerope\nYurocheosaurus\nCaacisan\nTrocelosaurus\nAbcloma\nTraichiaurus\nAgrsbachus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11400, Loss: 22.704922\n\nNgyuspergsaurus\nImda\nJyusparchiaterataps\nNbaadrrcegitas\nYuspcheroptor\nDabbssaeritas\nTrraperoptltitar\nAadrrha\nTodoeroshus\nAgrurachus\n\n\nIteration: 11500, Loss: 22.545493\n\nMexutognasaurus\nImacairus\nJyvusatermasaurus\nMacagpsaurus\nYuspcidon\nCaacosaurus\nTrocolosaurus\nAbbroma\nTodogosaurus\nAgpsacarosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11600, Loss: 22.610748\n\nMixustarasaurus\nIncabatroi\nJyutngonosaurus\nMacaiton\nYuspangis\nDaaltona\nTrodomosaurus\nA\nToendongthus\nAisteg\n\n\nIteration: 11700, Loss: 22.523440\n\nNgytosaurus\nJihabesaurus\nKustrgongonthus\nNacalosaurus\nXuspeomopguptor\nCaberthacisaurus\nTosangosaurus\nA\nToeneroptorax\nAistia\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11800, Loss: 22.529378\n\nMexusmanisaurus\nHeecaesaurus\nHypsocheosaurus\nMacaesopheroptopptor\nXutocemiodyn\nCabcosaurus\nTrocharoptorsia\nAacosaurus\nTocheosaurus\nAeropderus\n\n\nIteration: 11900, Loss: 22.418551\n\nMexusncheroptosaurus\nKoibaisaurus\nKustollisaurus\nMacairosaurus\nXusoelerantisaurus\nDabbosaurus\nTrocharopterix\nA\nToeliosaurus\nAhpona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12000, Loss: 22.502752\n\nMawustarchusuinus\nKiabaestee\nKustrasaurus\nMacagrus\nYusqanethaurus\nDaacosaurus\nToramasaurus\nA\nToenclodyititaproptochusus\nAgpsaderinus\n\n\nIteration: 12100, Loss: 22.551287\n\nMaxtytaong\nKiacalptom\nLutrodonasaurus\nMacaisteg\nXutognatong\nDaaestegansaurus\nTrochasogsaurus\nA\nToengtonthosaurus\nAhpticantitan\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12200, Loss: 22.433284\n\nMexushangosaurus\nJiccaesia\nKustrarhandus\nMacaisaurosaurus\nXuwpeonimasaurus\nDaberopechus\nToranesaurus\nA\nTichenopuraptes\nAhpria\n\n\nIteration: 12300, Loss: 22.513918\n\nMewustandophus\nKicbagssaggus\nLustuendosaurus\nMacairsaurus\nYuspanchraushus\nDabaropa\nTrichenleusaurus\nAbbosaurus\nToendopeusaurosaurus\nAlptedanthus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12400, Loss: 22.465461\n\nLexytolongenus\nHegaalosan\nHustraraptor\nLacalosamaus\nYusodonasaurus\nCabatona\nTorandorawoprochorase\nAbasscenator\nToeogoratops\nAistehanorophus\n\n\nIteration: 12500, Loss: 22.421678\n\nLevstoikanoroptor\nGigabasaurlisaurus\nHystrephopgsaurus\nLacalotelatops\nYusodonatetapsuchus\nBaacisatats\nTrodoloraterataus\nAbbosaurus\nTogleosaurus\nAistegatops\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12600, Loss: 22.372444\n\nNgwusodon\nKidabbosaurus\nKustriojon\nNacalosaurus\nXustbolosaurus\nCabbosaurus\nTrocimingus\nAbbrona\nTognanosaurus\nAgrtidanusaurus\n\n\nIteration: 12700, Loss: 22.453225\n\nLeussoerasaurus\nHegaaesaurus\nHussuanesaurus\nLacalosaurus\nYusnamasaurus\nCabatrolasaurus\nTosaursaurus\nAbasiacaratops\nTibemnosaurus\nAfpracaptesaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12800, Loss: 22.425174\n\nMextrogonosaurus\nHigbalosaurus\nIustrhonosaurus\nMadaistelagrus\nYusraposaurus\nDabbosaurus\nTrocheronthus\nAberria\nTrenishis\nAironechus\n\n\nIteration: 12900, Loss: 22.480025\n\nMawtosaurus\nHegaalosaurus\nIussnardin\nMaeadropedos\nYuspaniniaus\nCabarrga\nTrocheroptorax\nAbdosaurus\nToendrontesaurus\nAgropa\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13000, Loss: 22.429812\n\nMexuskeratops\nImacaersaurus\nJustoloposaurus\nMacaerosaurus\nYusodonkinthosaurus\nCabaropa\nTorapardix\nAbasolechylus\nTodolosaurus\nAgroracosaurus\n\n\nIteration: 13100, Loss: 22.369815\n\nMexusoenatorgiitoka\nJicaagrus\nKystrcongmaurus\nMacakosaurus\nYusrarasaurus\nDaaerria\nTrocheroptor\nAbispela\nToeohorbus\nAirpelantos\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13200, Loss: 22.322972\n\nMixtsner\nJicabaurus\nKusspaonosaurus\nMacafosaurus\nXusocendontis\nCaalosaurus\nTorangosaurus\nA\nTocelionthus\nAgrona\n\n\nIteration: 13300, Loss: 22.400547\n\nMexusnangmhaurus\nJiaeakosaurus\nKusureonkiaurus\nMacaerraceratops\nYuruangosaurus\nDaakpsaongvephalus\nTorangosaurus\nAafrradeosaurus\nToeneopasaurus\nAjrona\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13400, Loss: 22.266172\n\nMawusianisaurus\nImacalosaurus\nJutrokenititandun\nMacalosaurus\nYuspckerniusauraporesaurus\nCabasonebitis\nTrochanonus\nAbcosaurus\nTodocnonus\nAisoleestisaurus\n\n\nIteration: 13500, Loss: 22.367553\n\nMawusmalian\nJiadaltor\nKustraperatops\nMacajosaurus\nYuspancoraurus\nDaagrona\nTosangosaurus\nAbcosaurus\nTiemeronyptor\nAjpria\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13600, Loss: 22.336854\n\nMaxusolnethos\nJiacalosaurus\nKustrhmasaurus\nMacaisaurus\nYusogndosaurus\nDabarraphos\nToramisaurus\nA\nTogiaroptor\nAgropegltosaurus\n\n\nIteration: 13700, Loss: 22.280743\n\nMixustes\nKola\nKustreoriostes\nMacagrope\nYusteonosaurus\nDabcosaurus\nTrocheronus\nA\nTicheropteryx\nAistegatosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 13800, Loss: 22.340965\n\nMexuspandon\nJiceadosaurus\nKustsasaurus\nMacaisauriptes\nYuspandophus\nDabarophanthus\nTrocharontimus\nAbcosaurus\nTogmanncteros\nAlporacnus\n\n\nIteration: 13900, Loss: 22.294121\n\nMavuspanimicus\nHiacalosaurus\nIustrareos\nMacalosaurus\nYuspaonor\nCaadosaurus\nTrocheosaurus\nAblosaurus\nToeohorgys\nAlropeertetha\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14000, Loss: 22.200563\n\nLiutoreorepoteraunus\nHoedalosaurus\nHustperatops\nLacalosaurus\nYusodonasaurus\nCabasocemator\nTorankoraurus\nAbassuiaptor\nTododon\nAlosaurus\n\n\nIteration: 14100, Loss: 22.158785\n\nMexusiangosaurus\nHiceedosaurus\nHussuchiondylusaurus\nMacamosaurus\nYuspangosaurus\nCabarolachusaurus\nTrochimodus\nAbcosaurus\nTodocophuonitar\nAhosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14200, Loss: 22.331211\n\nMexstrbonmilus\nIlcabasria\nIustrariasaurus\nMacalrsaurus\nXossangosaurus\nCachosaurus\nTriciisaurus\nAbhosaurus\nTiangosaurus\nAhqshadrsaurus\n\n\nIteration: 14300, Loss: 22.304535\n\nMexstrhanosaurus\nHelaakosaurus\nIvuskapeng\nMadaerophchus\nYuspanetontasaurus\nDabauria\nTrocellopur\nAbdosaurus\nToeneronthus\nAirthaerus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14400, Loss: 22.339462\n\nMawuster\nHicabasaurosishurius\nIustrerasaurus\nMadadosaurus\nYussangosaurus\nCaacosaurus\nTrocheronthrmpaphia\nAbcosaurus\nToeneronusauris\nAgrscelititan\n\n\nIteration: 14500, Loss: 22.349444\n\nMexusoenatopterptater\nHoedalosaurus\nJuusodonathus\nMadadosatctynus\nYuspankosaurus\nDabaropelty\nTordolnosuorosaurus\nBaerondansaurus\nTodoenosteros\nAgrondanthus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14600, Loss: 22.259392\n\nLiwutodon\nHoedakosaurus\nHytosaurus\nLacalosaurus\nYutodon\nCabcoteg\nTrocheosaurus\nAbdosaurus\nTododon\nAgropa\n\n\nIteration: 14700, Loss: 22.248166\n\nMexuspenatops\nImacaesaurus\nJyusoceosaurus\nMacagosaurus\nYuspangosaurus\nCabassia\nTrodonosaurus\nAbdroncaprosaurus\nTododosaurus\nAgropa\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14800, Loss: 22.232630\n\nMixusoceosaurus\nKokabaspidanros\nKustokelis\nMacagrsandrus\nXutognapicuris\nDaadosaurus\nTorangosaurus\nA\nToenchonykus\nAgropa\n\n\nIteration: 14900, Loss: 22.174820\n\nMawusnanisaurus\nImacalosaurus\nJussoeongkitan\nMacalosaurus\nXutodon\nCaceroncansaurus\nTrochanodun\nAbbosaurus\nTodogosaurus\nAgroncansaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15000, Loss: 22.157801\n\nMawuromeklons\nKoia\nKususaukosaurus\nMacaisceiatordia\nXusranisaurus\nDabasrag\nToranisaurus\nAbasrag\nTichgnketatos\nAispeichyiten\n\n\nIteration: 15100, Loss: 22.214705\n\nMawurolia\nJola\nKustraphosaurus\nMacairopharsaurus\nYuspandophus\nCabasonda\nTorapaosaurus\nA\nTichaoravisaurus\nAgpondansaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15200, Loss: 22.195036\n\nNgyuspandus\nKolaaeseurosaurus\nLytrsaurus\nNacalosaurus\nYusqanetia\nDabapopa\nTrocharomus\nA\nToindosaurus\nAjprha\n\n\nIteration: 15300, Loss: 22.154113\n\nMixstomicuphurosaurus\nJiaeaesia\nKutosaurus\nMacaeropabititan\nXutodon\nDaagosaurus\nTosaterantosaurus\nAbaropa\nToelanonurlos\nAipoma\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15400, Loss: 22.165424\n\nMextsterisaurus\nJiceadosaurus\nKustrckeosaurus\nMacaeropedosaurus\nYuspdonnopuratodsaurus\nDabeosaurus\nTrocheosaurus\nAbersaurus\nToglclohur\nAipsederus\n\n\nIteration: 15500, Loss: 22.108200\n\nMexustepmagptereu\nImadansaurorus\nJususaurus\nMadalosaurus\nYussangosaurus\nCabastelania\nTrocepkiaus\nAbcosaurus\nTogolopeus\nAlsteiangosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15600, Loss: 22.032737\n\nLiutosaurus\nHuedalosaurus\nHusttisaurus\nLecalotia\nYutodon\nCabcoten\nTrodokiiaurus\nAbcoten\nTogikoraurus\nAisunachus\n\n\nIteration: 15700, Loss: 22.069796\n\nLiutosaurus\nHicabasaurus\nHustocheprius\nLbaacloia\nYussanchuhus\nCabbosaurus\nToranglolus\nAbcosaurus\nTicoipceusaurus\nAgrodakus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15800, Loss: 22.184713\n\nMexvurephosaurus\nIngcamosaurus\nJussulongniteraterasauris\nMadaestecamps\nYushangosaurus\nDabbrrhadosaurus\nTrochisaurus\nBaistheenta\nTodoforatorntaps\nAgrsceirus\n\n\nIteration: 15900, Loss: 22.194676\n\nMexstoeomimus\nHibaaepsaurus\nHyrosaurus\nMacagosaurus\nYusodiadieus\nCabaroodanushus\nTrocheosaurus\nAbcosaurus\nToeneronymus\nAgrondeotitan\n\n\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters, iteration, losses = model(data, index_to_char, char_to_index)\n",
    "plt.plot(iteration, losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
